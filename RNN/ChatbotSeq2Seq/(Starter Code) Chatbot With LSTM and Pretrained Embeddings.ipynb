{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.Data import loadDF, tokenizer, getPairs, add_symbols, create_word_embedding, add_symbols2\n",
    "from src.Models import Seq2Seq, Encoder, Decoder\n",
    "from src.Vocab import Vocab\n",
    "from src.Train import train\n",
    "from src.Evaluate import evaluate\n",
    "from src.Chat import chat\n",
    "from src.ValEarlyStop import ValidationLossEarlyStopping\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random, math, time\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f94793452f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\", \"The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\", \"The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "print([\" \".join(sent) for sent in brown.sents()[0:3]])\n",
    "\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "# The default value of vector_size is 100.\n",
    "# model = gensim.models.Word2Vec(brown.sents(), size=100)\n",
    "# model.save('brown.embedding')\n",
    "\n",
    "# i can use googlenews vector which doesn't need training but it is very very large\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/notebooks/udacity/RNN/ChatBot_Seq2Seq/src/Data.py:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  return train_df.append(validation_df)\n"
     ]
    }
   ],
   "source": [
    "data_df = loadDF('data')\n",
    "# I will take only the first 5,000 Q&A to avoid CUDA out of memory error due to the large dataset\n",
    "data_df = data_df.iloc[:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4983</td>\n",
       "      <td>3642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Who was Alexander Scriabin's teacher?</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Question     Answer\n",
       "count                                    5000       5000\n",
       "unique                                   4983       3642\n",
       "top     Who was Alexander Scriabin's teacher?  Manhattan\n",
       "freq                                        2         21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                    Answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724441/4135065291.py:1: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
      "  data_df['Question'], data_df['Qtoken'] = data_df['Question'].apply(tokenizer).str\n",
      "/tmp/ipykernel_724441/4135065291.py:2: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
      "  data_df['Answer'], data_df['Atoken'] = data_df['Answer'].apply(tokenizer).str\n"
     ]
    }
   ],
   "source": [
    "data_df['Question'], data_df['Qtoken'] = data_df['Question'].apply(tokenizer).str\n",
    "data_df['Answer'], data_df['Atoken'] = data_df['Answer'].apply(tokenizer).str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Qtoken</th>\n",
       "      <th>Atoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to whom did the virgin mari alleg appear in 18...</td>\n",
       "      <td>saint bernadett soubir</td>\n",
       "      <td>[to, whom, did, the, virgin, mari, alleg, appe...</td>\n",
       "      <td>[saint, bernadett, soubir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is in front of the notr dame main build</td>\n",
       "      <td>a copper statu of christ</td>\n",
       "      <td>[what, is, in, front, of, the, notr, dame, mai...</td>\n",
       "      <td>[a, copper, statu, of, christ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the basilica of the sacr heart at notr dame is...</td>\n",
       "      <td>the main build</td>\n",
       "      <td>[the, basilica, of, the, sacr, heart, at, notr...</td>\n",
       "      <td>[the, main, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is the grotto at notr dame</td>\n",
       "      <td>a marian place of prayer and reflect</td>\n",
       "      <td>[what, is, the, grotto, at, notr, dame]</td>\n",
       "      <td>[a, marian, place, of, prayer, and, reflect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what sit on top of the main build at notr dame</td>\n",
       "      <td>a golden statu of the virgin mari</td>\n",
       "      <td>[what, sit, on, top, of, the, main, build, at,...</td>\n",
       "      <td>[a, golden, statu, of, the, virgin, mari]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  to whom did the virgin mari alleg appear in 18...   \n",
       "1       what is in front of the notr dame main build   \n",
       "2  the basilica of the sacr heart at notr dame is...   \n",
       "3                    what is the grotto at notr dame   \n",
       "4     what sit on top of the main build at notr dame   \n",
       "\n",
       "                                 Answer  \\\n",
       "0                saint bernadett soubir   \n",
       "1              a copper statu of christ   \n",
       "2                        the main build   \n",
       "3  a marian place of prayer and reflect   \n",
       "4     a golden statu of the virgin mari   \n",
       "\n",
       "                                              Qtoken  \\\n",
       "0  [to, whom, did, the, virgin, mari, alleg, appe...   \n",
       "1  [what, is, in, front, of, the, notr, dame, mai...   \n",
       "2  [the, basilica, of, the, sacr, heart, at, notr...   \n",
       "3            [what, is, the, grotto, at, notr, dame]   \n",
       "4  [what, sit, on, top, of, the, main, build, at,...   \n",
       "\n",
       "                                         Atoken  \n",
       "0                    [saint, bernadett, soubir]  \n",
       "1                [a, copper, statu, of, christ]  \n",
       "2                            [the, main, build]  \n",
       "3  [a, marian, place, of, prayer, and, reflect]  \n",
       "4     [a, golden, statu, of, the, virgin, mari]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['to whom did the virgin mari alleg appear in 1858 in lourd franc',\n",
       "  'saint bernadett soubir'],\n",
       " ['what is in front of the notr dame main build', 'a copper statu of christ'],\n",
       " ['the basilica of the sacr heart at notr dame is besid to which structur',\n",
       "  'the main build'],\n",
       " ['what is the grotto at notr dame', 'a marian place of prayer and reflect'],\n",
       " ['what sit on top of the main build at notr dame',\n",
       "  'a golden statu of the virgin mari']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_sequence = getPairs(data_df)\n",
    "first_five_items = pairs_sequence[:5]\n",
    "# import itertools\n",
    "# first_five_items = list(itertools.islice(pairs_sequence, 5))\n",
    "print(len(pairs_sequence))\n",
    "first_five_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_src, max_trg = getMaxLen(pairs_sequence)\n",
    "# max_src, max_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total of unique questions and answers in dataset:  8505\n",
      "raw-vocab: 6321\n",
      "vocab-length: 6324\n",
      "word2idx-length: 6324\n",
      "{0: '<pad>', 1: '<sos>', 2: '<eos>', 3: 'the', 4: 'what', 5: 'of', 6: 'in', 7: 'did', 8: 'was', 9: 'to'}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'the': 3, 'what': 4, 'of': 5, 'in': 6, 'did': 7, 'was': 8, 'to': 9}\n",
      "['<pad>', '<sos>', '<eos>', 'the', 'what', 'of', 'in', 'did', 'was', 'to']\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_vocab = Vocab(data_df)\n",
    "print(\"total of unique questions and answers in dataset: \", len(data_vocab.text))\n",
    "# A_vocab = Vocab()\n",
    "\n",
    "data_vocab.build_word_vocab()\n",
    "\n",
    "print({k: data_vocab.index2word[k] for k in list(data_vocab.index2word)[:10]})\n",
    "print({k: data_vocab.word2index[k] for k in list(data_vocab.word2index)[:10]})\n",
    "print(data_vocab.word_vocab[:10])\n",
    "print(data_vocab['the'])\n",
    "print(data_vocab['oov'])\n",
    "\n",
    "# # build vocabularies for questions \"source\" and answers \"target\"\n",
    "# for pair in pairs_sequence:\n",
    "#     Q_vocab.add_words(pair[0])\n",
    "#     A_vocab.add_words(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total of unique questions in dataset:  4980\n",
      "raw-vocab: 4504\n",
      "vocab-length: 4507\n",
      "word2idx-length: 4507\n",
      "total of unique answers in dataset:  3525\n",
      "raw-vocab: 4081\n",
      "vocab-length: 4084\n",
      "word2idx-length: 4084\n"
     ]
    }
   ],
   "source": [
    "src_data_vocab = Vocab(data_df, source=True)\n",
    "print(\"total of unique questions in dataset: \", len(src_data_vocab.text))\n",
    "# A_vocab = Vocab()\n",
    "\n",
    "src_data_vocab.build_word_vocab()\n",
    "\n",
    "trg_data_vocab = Vocab(data_df, source=False)\n",
    "print(\"total of unique answers in dataset: \", len(trg_data_vocab.text))\n",
    "# A_vocab = Vocab()\n",
    "\n",
    "trg_data_vocab.build_word_vocab()\n",
    "\n",
    "# # build vocabularies for questions \"source\" and answers \"target\"\n",
    "# for pair in pairs_sequence:\n",
    "#     Q_vocab.add_words(pair[0])\n",
    "#     A_vocab.add_words(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchdata.datapipes.iter import IterableWrapper\n",
    "# tmp = IterableWrapper(pairs_sequence).sharding_filter()\n",
    "# a, b = next(iter(tmp))\n",
    "# print(a)\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source_data = [toTensor(data_vocab, pair[0]) for pair in pairs_sequence]\n",
    "# target_data = [toTensor(data_vocab, pair[1]) for pair in pairs_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(source_data[10].shape)\n",
    "# print(source_data[0].view(-1).shape)\n",
    "# print(source_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix, words_found = create_word_embedding(w2v.wv, word_vocab=src_data_vocab.word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words found in glove vocab: 0 from 6324\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words found in glove vocab: {0} from {1}\".format(words_found, len(data_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('seq2seqEmb_vt.npy', weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "practice to understand batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "print(data_vocab['whom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(source_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_batches(tmp, batch_size, seq_length):\n",
    "\n",
    "#     n_batches = int(tmp.shape[0]/(batch_size*seq_length))\n",
    "#     print(n_batches)\n",
    "#     tmp = tmp[:n_batches*batch_size*seq_length]\n",
    "#     tmp = tmp.reshape(batch_size, -1)\n",
    "#     print(tmp.shape)\n",
    "#     print(tmp)\n",
    "#     print((tmp[0:2, :-1]).shape)\n",
    "#     ## now, we have to Iterate over the batches using a window of size seq_length\n",
    "#     for n in range(0, tmp.shape[1], seq_length):\n",
    "#         # The features\n",
    "#         x = tmp[:, n:n+seq_length]\n",
    "#         # The targets, shifted by one\n",
    "#         y = np.zeros_like(x)\n",
    "#         try:\n",
    "#             y[:, :-1], y[:, -1] = x[:, 1:], tmp[:, n+seq_length]\n",
    "#         except IndexError:\n",
    "#             y[:, :-1], y[:, -1] = x[:, 1:], tmp[:, 0]\n",
    "#         yield x, y\n",
    "\n",
    "# seq_length = 4\n",
    "# batch_size = 10\n",
    "# tmp = np.array([55, 20, 48, 54, 76, 36, 12,  4, 81,  7,  7,  7, 57, 48, 54, 54, 65,\n",
    "#         4, 66, 48, 69, 78,  9, 78, 36, 19,  4, 48, 12, 36,  4, 48,  9,  9,\n",
    "#         4, 48,  9, 78, 17, 36, 47,  4, 36, 27, 36, 12, 65,  4, 44, 29, 20,\n",
    "#        48, 54, 54, 65,  4, 66, 48, 69, 78,  9, 65,  4, 78, 19,  4, 44, 29,\n",
    "#        20, 48, 54, 54, 65,  4, 78, 29,  4, 78, 76, 19,  4, 18, 46, 29,  7,\n",
    "#        46, 48, 65,  0,  7,  7, 52, 27, 36, 12, 65, 76, 20, 78, 29, 213, 4])\n",
    "# i = 0\n",
    "# for d in (get_batches(tmp, batch_size, seq_length)):\n",
    "#     i +=1\n",
    "# print(f\"The number of batches (iterations): {i}\")\n",
    "# # next(get_batches(tmp, batch_size, seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kf = KFold(n_splits=10, shuffle=True)\n",
    "# tmp = source_data[:20]\n",
    "# for e, (train_index, test_index) in enumerate(kf.split(tmp), 1):\n",
    "#     print(f\"Iteration: {e}\")\n",
    "#     print(f\"{train_index}->{len(train_index)}\")\n",
    "#     print(f\"{test_index}->{len(test_index)}\")\n",
    "\n",
    "    \n",
    "#     # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler()])\n",
    "\n",
    "\n",
    "def generate_batch(batch):\n",
    "    src_batch = []\n",
    "    trg_batch = []\n",
    "    src_len = []\n",
    "    i = 0\n",
    "    # print(type(batch))\n",
    "    for src, trg in batch:\n",
    "        i += 1\n",
    "        #split sentence into tokens\n",
    "        _, src_tokens = tokenizer(src)\n",
    "        # logging.warning(f'iteration {i}:\\n {src}'); # why prints 3 times while batch is 1?\n",
    "        _, trg_tokens = tokenizer(trg)\n",
    "        #convert tokens to index and to tensor and add <sos> and <eos> to each sentence\n",
    "        src_tensor = add_symbols(torch.tensor(src_data_vocab(src_tokens)).long(), src_data_vocab)\n",
    "        trg_tensor = add_symbols2(torch.tensor(trg_data_vocab(trg_tokens)).long(), trg_data_vocab)\n",
    "        src_batch.append(src_tensor)\n",
    "        #track length of each source sentence, not useful in this model. Will be useful in further models\n",
    "        src_len.append(len(src_tensor))\n",
    "        trg_batch.append(trg_tensor)\n",
    "        # logging.warning(f'iteration {i}:\\n {(src_tensor)}');\n",
    "    src_len = torch.tensor(src_len, dtype = torch.int)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=src_data_vocab['<pad>'])\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=trg_data_vocab['<pad>'])\n",
    "    src_len, idx = torch.sort(src_len,descending=True)\n",
    "    #src_len is not useful in this model\n",
    "    # logging.warning(f'lsrc_batch:{len(src_batch)}')\n",
    "    return src_batch, src_len, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(pairs_sequence, batch_size=5, collate_fn=generate_batch)\n",
    "sr, srlen, tg = (next(iter(train_dataloader)))\n",
    "print(len(srlen))\n",
    "# print(len(list(train_dataloader))) # it is grouped to 5 items per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    1,    1,    1,    1],\n",
       "        [   9,    4,    3,    4,    4],\n",
       "        [ 155,   11, 1555,   11, 1288],\n",
       "        [   7,    6,    5,    3,   22],\n",
       "        [   3, 1287,    3, 1945,  338],\n",
       "        [2666,    5, 1556,   26,    5],\n",
       "        [1942,    3, 1085,   35,    3],\n",
       "        [1943,   35,   26,   34,  242],\n",
       "        [ 234,   34,   35,    2,   88],\n",
       "        [   6,  242,   34,    0,   26],\n",
       "        [2667,   88,   11,    0,   35],\n",
       "        [   6,    2,  409,    0,   34],\n",
       "        [1944,    0,    9,    0,    2],\n",
       "        [1084,    0,   14,    0,    0],\n",
       "        [   2,    0,  549,    0,    0],\n",
       "        [   0,    0,    2,    0,    0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 483,    6,    3,    6,    6],\n",
       "        [1436,  738,  740, 1439, 1441],\n",
       "        [1437,  739,  156,  741,  739],\n",
       "        [   2,    5,    2,    5,    5],\n",
       "        [   0, 1438,    0,  742,    3],\n",
       "        [   0,    2,    0,    4,  743],\n",
       "        [   0,    0,    0, 1440,  256],\n",
       "        [   0,    0,    0,    2,    2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time/60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins*60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "INPUT_DIM = len(src_data_vocab)\n",
    "OUTPUT_DIM = len(trg_data_vocab)\n",
    "ENC_EMB_DIM = 128 # 256//(2*2)\n",
    "DEC_EMB_DIM = 128 # 256//(2*2)\n",
    "HID_DIM = 128\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "RNN_DROPOUT = 0.0\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, RNN_DROPOUT, weights_matrix)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, RNN_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4507, 128)\n",
       "    (rnn): LSTM(128, 128)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(4084, 128)\n",
       "    (rnn): LSTM(128, 128)\n",
       "    (fc_out): Linear(in_features=128, out_features=4084, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if not name.startswith('encoder.embedding'):  # Exclude encoder embedding parameters\n",
    "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "            # nn.init.zeros_(param.data)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,890,676 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# optimizer = optim.Adamax(model.parameters())\n",
    "\n",
    "# ignore the loss whenever the target token is a padding token.\n",
    "TRG_PAD_IDX = trg_data_vocab['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After multiple training using the adam optimizer and batch of 12 and 10 folds\n",
    "# I have changed the batch size to 128\n",
    "# changed the optimizer to sgd\n",
    "# changed the fold to 20\n",
    "\n",
    "# finally i have changed the lr in sgd from 0.01 to 0.1 but did not train to the end. i just saved the model\n",
    "# after reloading the model, trained it for 40 epochs.\n",
    "# then changed rhe optimizer to adamax with default learning rate\n",
    "\n",
    "# model.load_state_dict(torch.load('seq2seq_adam_stemmer_brown_embedding.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.95it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 4s\n",
      "\tTrain Loss: 1.106 | Train PPL:   3.022\n",
      "\t Val. Loss: 1.059 |  Val. PPL:   2.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 17.31it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 30.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 4s\n",
      "\tTrain Loss: 1.045 | Train PPL:   2.843\n",
      "\t Val. Loss: 1.107 |  Val. PPL:   3.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 17.11it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 4s\n",
      "\tTrain Loss: 1.007 | Train PPL:   2.738\n",
      "\t Val. Loss: 1.154 |  Val. PPL:   3.171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.70it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 22.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 4s\n",
      "\tTrain Loss: 0.984 | Train PPL:   2.674\n",
      "\t Val. Loss: 1.197 |  Val. PPL:   3.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.98it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 4s\n",
      "\tTrain Loss: 0.965 | Train PPL:   2.624\n",
      "\t Val. Loss: 1.233 |  Val. PPL:   3.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.62it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 23.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 4s\n",
      "\tTrain Loss: 0.948 | Train PPL:   2.580\n",
      "\t Val. Loss: 1.272 |  Val. PPL:   3.567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.78it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 27.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0m 4s\n",
      "\tTrain Loss: 0.936 | Train PPL:   2.549\n",
      "\t Val. Loss: 1.306 |  Val. PPL:   3.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.86it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 24.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0m 4s\n",
      "\tTrain Loss: 0.916 | Train PPL:   2.499\n",
      "\t Val. Loss: 1.336 |  Val. PPL:   3.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 17.08it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 37.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0m 4s\n",
      "\tTrain Loss: 0.909 | Train PPL:   2.483\n",
      "\t Val. Loss: 1.363 |  Val. PPL:   3.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.58it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 24.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0m 4s\n",
      "\tTrain Loss: 0.894 | Train PPL:   2.444\n",
      "\t Val. Loss: 1.391 |  Val. PPL:   4.019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 24.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 0m 4s\n",
      "\tTrain Loss: 0.891 | Train PPL:   2.438\n",
      "\t Val. Loss: 1.418 |  Val. PPL:   4.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 17.07it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 30.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 0m 4s\n",
      "\tTrain Loss: 0.883 | Train PPL:   2.417\n",
      "\t Val. Loss: 1.440 |  Val. PPL:   4.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time: 0m 4s\n",
      "\tTrain Loss: 0.871 | Train PPL:   2.390\n",
      "\t Val. Loss: 1.466 |  Val. PPL:   4.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.52it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 24.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time: 0m 4s\n",
      "\tTrain Loss: 0.865 | Train PPL:   2.375\n",
      "\t Val. Loss: 1.488 |  Val. PPL:   4.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 16.68it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 32.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time: 0m 4s\n",
      "\tTrain Loss: 0.858 | Train PPL:   2.358\n",
      "\t Val. Loss: 1.514 |  Val. PPL:   4.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [00:04<00:00, 17.45it/s]\n",
      "Evaluation: 100%|██████████| 4/4 [00:00<00:00, 30.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Time: 0m 4s\n",
      "\tTrain Loss: 0.853 | Train PPL:   2.347\n",
      "\t Val. Loss: 1.535 |  Val. PPL:   4.643\n",
      "Repeated slow change in validation loss for 15 times.\n",
      "Early stopping at epoch 16 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 40\n",
    "CLIP = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# speedup the training by reducing the size to grasp how the model is doing\n",
    "half_length = len(pairs_sequence) // 1\n",
    "cut_list = pairs_sequence[:half_length]\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "\n",
    "# Lists to store performance metrics for each fold\n",
    "fold_metrics = []\n",
    "\n",
    "\n",
    "# Loop through each fold\n",
    "for fold_x, (train_indices, val_indices) in enumerate(kf.split(cut_list), 1):\n",
    "    train_data = torch.utils.data.Subset(cut_list, train_indices)\n",
    "    val_data = torch.utils.data.Subset(cut_list, val_indices)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    \n",
    "    early_stop = ValidationLossEarlyStopping(patience=15, min_delta=0.0001)\n",
    "    best_val_loss = float('inf')\n",
    "    # Training/Validation loop\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, answer_token = train(model, train_dataloader, optimizer, criterion, CLIP, trg_data_vocab)\n",
    "        val_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'seq2seq_fold_{fold_x:02}.pt')\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
    "        \n",
    "        if(early_stop(val_loss)):\n",
    "            print(f\"Repeated slow change in validation loss for {early_stop.patience} times.\")\n",
    "            print(f\"Early stopping at epoch {epoch+1:02} ...\")\n",
    "            # print(f\"The last output of the decoder: {answer_token}\")\n",
    "            break # let \n",
    "    fold_metrics.append(val_loss)\n",
    "    break\n",
    "    # create new model for the next fold\n",
    "    # model = Seq2Seq(enc, dec, device).to(device)\n",
    "    # model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'seq2seq_adam_stemmer_brown_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 1.54\n",
      "Standard Deviation: 0.00\n",
      "All folds: [1.535272091627121]\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and standard deviation of the accuracy across folds\n",
    "mean_accuracy = sum(fold_metrics) / len(fold_metrics)\n",
    "std_accuracy = (sum((x - mean_accuracy) ** 2 for x in fold_metrics) / len(fold_metrics)) ** 0.5\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.2f}\")\n",
    "print(f\"Standard Deviation: {std_accuracy:.2f}\")\n",
    "print(f\"All folds: {fold_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, RNN_DROPOUT, weights_matrix)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, RNN_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "model.load_state_dict(torch.load('seq2seq_fold_01.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what is the grotto at notre-dame?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3328, 12, 4, 2]\n",
      "< wen for and \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  to whom did the virgin mary allegedly appear in 1858 in lourdes france\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "<  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    src = input(\"> \")\n",
    "    if src.strip() == \"exit\":\n",
    "        break\n",
    "    chat(src, trg_data_vocab, model, 10)\n",
    "    \n",
    "# to whom did the virgin mary allegedly appear in 1858 in lourdes france\n",
    "# what is the grotto at notre-dame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 - GPU (ipykernel)",
   "language": "python",
   "name": "python3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
