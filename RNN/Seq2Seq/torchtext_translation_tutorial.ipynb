{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Ucw3T7P0yuhj"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C78eYbd6yuhl"
   },
   "source": [
    "\n",
    "Language Translation with TorchText\n",
    "===================================\n",
    "\n",
    "This tutorial shows how to use ``torchtext`` to preprocess\n",
    "data from a well-known dataset containing sentences in both English and German and use it to\n",
    "train a sequence-to-sequence model with attention that can translate German sentences\n",
    "into English.\n",
    "\n",
    "It is based off of\n",
    "`this tutorial <https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__\n",
    "from PyTorch community member `Ben Trevett <https://github.com/bentrevett>`__\n",
    "with Ben's permission. We update the tutorials by removing some legacy code.\n",
    "\n",
    "By the end of this tutorial, you will be able to preprocess sentences into tensors for NLP modeling and use `torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__ for training and validing the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iF6KweByuhn"
   },
   "source": [
    "Data Processing\n",
    "----------------\n",
    "``torchtext`` has utilities for creating datasets that can be easily\n",
    "iterated through for the purposes of creating a language translation\n",
    "model. In this example, we show how to tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor.\n",
    "\n",
    "Note: the tokenization in this tutorial requires `Spacy <https://spacy.io>`__\n",
    "We use Spacy because it provides strong support for tokenization in languages\n",
    "other than English. ``torchtext`` provides a ``basic_english`` tokenizer\n",
    "and supports other tokenizers for English (e.g.\n",
    "`Moses <https://bitbucket.org/luismsgomes/mosestokenizer/src/default/>`__)\n",
    "but for language translation - where multiple languages are required -\n",
    "Spacy is your best bet.\n",
    "\n",
    "To run this tutorial, first install ``spacy`` using ``pip`` or ``conda``.\n",
    "Next, download the raw data for the English and German Spacy tokenizers:\n",
    "\n",
    "::\n",
    "\n",
    "   python -m spacy download en\n",
    "   python -m spacy download de\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gH9i1wpfyuhn"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_))\n",
    "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "de_vocab.set_default_index(de_vocab['<unk>'])\n",
    "en_vocab.set_default_index(en_vocab['<unk>'])\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]),\n",
       "  tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])),\n",
       " (tensor([18,  7, 19, 20, 21, 22, 23, 16, 17]),\n",
       "  tensor([16, 17, 18, 19, 20,  9, 21, 22, 23, 24, 25, 14, 15])),\n",
       " (tensor([24, 25, 26, 27, 11, 22, 28, 29, 30, 16, 17]),\n",
       "  tensor([26, 27, 28, 29, 30, 22, 31, 32, 14, 15])),\n",
       " (tensor([24, 31, 11, 32, 33, 34, 35, 36, 37, 38, 39, 40, 22, 41, 16, 17]),\n",
       "  tensor([26, 33, 18, 22, 34, 35, 36, 37, 38, 22, 39, 40, 22, 41, 14, 15])),\n",
       " (tensor([ 4,  7, 42, 43, 44, 39, 45, 46, 47, 16, 17]),\n",
       "  tensor([ 4, 17,  9, 42, 43, 44, 45, 46, 14, 15]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmriO41-yuhn"
   },
   "source": [
    "``DataLoader``\n",
    "----------------\n",
    "The last ``torch`` specific feature we'll use is the ``DataLoader``,\n",
    "which is easy to use since it takes the data as its\n",
    "first argument. Specifically, as the docs say:\n",
    "``DataLoader`` combines a dataset and a sampler, and provides an iterable over the given dataset. The ``DataLoader`` supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.\n",
    "\n",
    "Please pay attention to ``collate_fn`` (optional) that merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tVkXdlMgyuho"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDGEfEYeyuho"
   },
   "source": [
    "Defining our ``nn.Module`` and ``Optimizer``\n",
    "----------------\n",
    "That's mostly it from a ``torchtext`` perspecive: with the dataset built\n",
    "and the iterator defined, the rest of this tutorial simply defines our\n",
    "model as an ``nn.Module``, along with an ``Optimizer``, and then trains it.\n",
    "\n",
    "Our model specifically, follows the architecture described\n",
    "`here <https://arxiv.org/abs/1409.0473>`__ (you can find a\n",
    "significantly more commented version\n",
    "`here <https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__).\n",
    "\n",
    "Note: this model is just an example model that can be used for language\n",
    "translation; we choose it because it is a standard model for the task,\n",
    "not because it is the recommended model to use for translation. As you're\n",
    "likely aware, state-of-the-art models are currently based on Transformers;\n",
    "you can see PyTorch's capabilities for implementing Transformer layers\n",
    "`here <https://pytorch.org/docs/stable/nn.html#transformer-layers>`__; and\n",
    "in particular, the \"attention\" used in the model below is different from\n",
    "the multi-headed self-attention present in a transformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Gl4Hm7bHyuho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,491,070 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim = 1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBXF0I1xyuhp"
   },
   "source": [
    "Note: when scoring the performance of a language translation model in\n",
    "particular, we have to tell the ``nn.CrossEntropyLoss`` function to\n",
    "ignore the indices where the target is simply padding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(19215, 32)\n",
       "    (rnn): GRU(32, 64, bidirectional=True)\n",
       "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(10838, 32)\n",
       "    (rnn): GRU(160, 64)\n",
       "    (out): Linear(in_features=224, out_features=10838, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dg9BJMt9yuhp"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = en_vocab['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7NPuxWWyuhq"
   },
   "source": [
    "Finally, we can train and evaluate this model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "jZ1lsmAlyuhq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 32s\n",
      "\tTrain Loss: 3.752 | Train PPL:  42.625\n",
      "\t Val. Loss: 4.420 |  Val. PPL:  83.077\n",
      "Epoch: 02 | Time: 0m 32s\n",
      "\tTrain Loss: 3.681 | Train PPL:  39.692\n",
      "\t Val. Loss: 4.379 |  Val. PPL:  79.768\n",
      "Epoch: 03 | Time: 0m 40s\n",
      "\tTrain Loss: 3.617 | Train PPL:  37.225\n",
      "\t Val. Loss: 4.349 |  Val. PPL:  77.377\n",
      "Epoch: 04 | Time: 0m 53s\n",
      "\tTrain Loss: 3.533 | Train PPL:  34.228\n",
      "\t Val. Loss: 4.303 |  Val. PPL:  73.933\n",
      "Epoch: 05 | Time: 0m 52s\n",
      "\tTrain Loss: 3.465 | Train PPL:  31.962\n",
      "\t Val. Loss: 4.228 |  Val. PPL:  68.581\n",
      "Epoch: 06 | Time: 0m 53s\n",
      "\tTrain Loss: 3.408 | Train PPL:  30.199\n",
      "\t Val. Loss: 4.174 |  Val. PPL:  64.955\n",
      "Epoch: 07 | Time: 0m 53s\n",
      "\tTrain Loss: 3.317 | Train PPL:  27.576\n",
      "\t Val. Loss: 4.144 |  Val. PPL:  63.069\n",
      "Epoch: 08 | Time: 0m 53s\n",
      "\tTrain Loss: 3.238 | Train PPL:  25.495\n",
      "\t Val. Loss: 4.100 |  Val. PPL:  60.317\n",
      "Epoch: 09 | Time: 0m 52s\n",
      "\tTrain Loss: 3.191 | Train PPL:  24.316\n",
      "\t Val. Loss: 4.079 |  Val. PPL:  59.094\n",
      "Epoch: 10 | Time: 0m 54s\n",
      "\tTrain Loss: 3.110 | Train PPL:  22.422\n",
      "\t Val. Loss: 4.068 |  Val. PPL:  58.437\n",
      "Epoch: 11 | Time: 0m 53s\n",
      "\tTrain Loss: 3.076 | Train PPL:  21.667\n",
      "\t Val. Loss: 4.011 |  Val. PPL:  55.199\n",
      "Epoch: 12 | Time: 0m 52s\n",
      "\tTrain Loss: 2.995 | Train PPL:  19.981\n",
      "\t Val. Loss: 3.980 |  Val. PPL:  53.539\n",
      "Epoch: 13 | Time: 0m 55s\n",
      "\tTrain Loss: 2.952 | Train PPL:  19.135\n",
      "\t Val. Loss: 3.922 |  Val. PPL:  50.500\n",
      "Epoch: 14 | Time: 0m 54s\n",
      "\tTrain Loss: 2.901 | Train PPL:  18.195\n",
      "\t Val. Loss: 3.902 |  Val. PPL:  49.503\n",
      "Epoch: 15 | Time: 0m 53s\n",
      "\tTrain Loss: 2.867 | Train PPL:  17.588\n",
      "\t Val. Loss: 3.869 |  Val. PPL:  47.915\n",
      "Epoch: 16 | Time: 0m 52s\n",
      "\tTrain Loss: 2.813 | Train PPL:  16.667\n",
      "\t Val. Loss: 3.842 |  Val. PPL:  46.628\n",
      "Epoch: 17 | Time: 0m 53s\n",
      "\tTrain Loss: 2.779 | Train PPL:  16.108\n",
      "\t Val. Loss: 3.817 |  Val. PPL:  45.458\n",
      "Epoch: 18 | Time: 0m 53s\n",
      "\tTrain Loss: 2.730 | Train PPL:  15.335\n",
      "\t Val. Loss: 3.798 |  Val. PPL:  44.590\n",
      "Epoch: 19 | Time: 0m 52s\n",
      "\tTrain Loss: 2.696 | Train PPL:  14.815\n",
      "\t Val. Loss: 3.781 |  Val. PPL:  43.871\n",
      "Epoch: 20 | Time: 0m 53s\n",
      "\tTrain Loss: 2.655 | Train PPL:  14.231\n",
      "\t Val. Loss: 3.771 |  Val. PPL:  43.423\n",
      "Epoch: 21 | Time: 0m 52s\n",
      "\tTrain Loss: 2.610 | Train PPL:  13.602\n",
      "\t Val. Loss: 3.790 |  Val. PPL:  44.255\n",
      "Epoch: 22 | Time: 0m 53s\n",
      "\tTrain Loss: 2.573 | Train PPL:  13.108\n",
      "\t Val. Loss: 3.796 |  Val. PPL:  44.538\n",
      "Epoch: 23 | Time: 0m 52s\n",
      "\tTrain Loss: 2.534 | Train PPL:  12.599\n",
      "\t Val. Loss: 3.749 |  Val. PPL:  42.490\n",
      "Epoch: 24 | Time: 0m 51s\n",
      "\tTrain Loss: 2.522 | Train PPL:  12.455\n",
      "\t Val. Loss: 3.743 |  Val. PPL:  42.238\n",
      "Epoch: 25 | Time: 0m 53s\n",
      "\tTrain Loss: 2.493 | Train PPL:  12.092\n",
      "\t Val. Loss: 3.761 |  Val. PPL:  43.011\n",
      "Epoch: 26 | Time: 0m 53s\n",
      "\tTrain Loss: 2.455 | Train PPL:  11.645\n",
      "\t Val. Loss: 3.734 |  Val. PPL:  41.859\n",
      "Epoch: 27 | Time: 0m 54s\n",
      "\tTrain Loss: 2.428 | Train PPL:  11.333\n",
      "\t Val. Loss: 3.749 |  Val. PPL:  42.484\n",
      "Epoch: 28 | Time: 0m 54s\n",
      "\tTrain Loss: 2.410 | Train PPL:  11.139\n",
      "\t Val. Loss: 3.721 |  Val. PPL:  41.325\n",
      "Epoch: 29 | Time: 0m 54s\n",
      "\tTrain Loss: 2.368 | Train PPL:  10.681\n",
      "\t Val. Loss: 3.716 |  Val. PPL:  41.089\n",
      "Epoch: 30 | Time: 0m 54s\n",
      "\tTrain Loss: 2.367 | Train PPL:  10.664\n",
      "\t Val. Loss: 3.727 |  Val. PPL:  41.553\n",
      "Epoch: 31 | Time: 0m 55s\n",
      "\tTrain Loss: 2.344 | Train PPL:  10.427\n",
      "\t Val. Loss: 3.697 |  Val. PPL:  40.315\n",
      "Epoch: 32 | Time: 0m 55s\n",
      "\tTrain Loss: 2.323 | Train PPL:  10.211\n",
      "\t Val. Loss: 3.737 |  Val. PPL:  41.971\n",
      "Epoch: 33 | Time: 0m 54s\n",
      "\tTrain Loss: 2.278 | Train PPL:   9.755\n",
      "\t Val. Loss: 3.744 |  Val. PPL:  42.265\n",
      "Epoch: 34 | Time: 0m 54s\n",
      "\tTrain Loss: 2.272 | Train PPL:   9.702\n",
      "\t Val. Loss: 3.710 |  Val. PPL:  40.858\n",
      "Epoch: 35 | Time: 0m 54s\n",
      "\tTrain Loss: 2.253 | Train PPL:   9.513\n",
      "\t Val. Loss: 3.729 |  Val. PPL:  41.618\n",
      "Epoch: 36 | Time: 0m 55s\n",
      "\tTrain Loss: 2.227 | Train PPL:   9.270\n",
      "\t Val. Loss: 3.735 |  Val. PPL:  41.907\n",
      "Epoch: 37 | Time: 0m 54s\n",
      "\tTrain Loss: 2.226 | Train PPL:   9.259\n",
      "\t Val. Loss: 3.736 |  Val. PPL:  41.910\n",
      "Epoch: 38 | Time: 0m 52s\n",
      "\tTrain Loss: 2.220 | Train PPL:   9.205\n",
      "\t Val. Loss: 3.718 |  Val. PPL:  41.194\n",
      "Epoch: 39 | Time: 0m 32s\n",
      "\tTrain Loss: 2.205 | Train PPL:   9.068\n",
      "\t Val. Loss: 3.724 |  Val. PPL:  41.431\n",
      "Epoch: 40 | Time: 0m 32s\n",
      "\tTrain Loss: 2.163 | Train PPL:   8.700\n",
      "\t Val. Loss: 3.749 |  Val. PPL:  42.462\n",
      "Epoch: 41 | Time: 0m 32s\n",
      "\tTrain Loss: 2.161 | Train PPL:   8.681\n",
      "\t Val. Loss: 3.720 |  Val. PPL:  41.274\n",
      "Epoch: 42 | Time: 0m 32s\n",
      "\tTrain Loss: 2.156 | Train PPL:   8.638\n",
      "\t Val. Loss: 3.738 |  Val. PPL:  42.024\n",
      "Epoch: 43 | Time: 0m 32s\n",
      "\tTrain Loss: 2.113 | Train PPL:   8.272\n",
      "\t Val. Loss: 3.744 |  Val. PPL:  42.277\n",
      "Epoch: 44 | Time: 0m 32s\n",
      "\tTrain Loss: 2.131 | Train PPL:   8.419\n",
      "\t Val. Loss: 3.732 |  Val. PPL:  41.782\n",
      "Epoch: 45 | Time: 0m 32s\n",
      "\tTrain Loss: 2.111 | Train PPL:   8.253\n",
      "\t Val. Loss: 3.752 |  Val. PPL:  42.586\n",
      "Epoch: 46 | Time: 0m 33s\n",
      "\tTrain Loss: 2.098 | Train PPL:   8.150\n",
      "\t Val. Loss: 3.712 |  Val. PPL:  40.938\n",
      "Epoch: 47 | Time: 0m 32s\n",
      "\tTrain Loss: 2.078 | Train PPL:   7.988\n",
      "\t Val. Loss: 3.731 |  Val. PPL:  41.717\n",
      "Epoch: 48 | Time: 0m 32s\n",
      "\tTrain Loss: 2.044 | Train PPL:   7.721\n",
      "\t Val. Loss: 3.772 |  Val. PPL:  43.487\n",
      "Epoch: 49 | Time: 0m 33s\n",
      "\tTrain Loss: 2.033 | Train PPL:   7.635\n",
      "\t Val. Loss: 3.769 |  Val. PPL:  43.357\n",
      "Epoch: 50 | Time: 0m 32s\n",
      "\tTrain Loss: 2.037 | Train PPL:   7.671\n",
      "\t Val. Loss: 3.763 |  Val. PPL:  43.067\n",
      "| Test Loss: 3.700 | Test PPL:  40.439 |\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'pytorch-translation_tutorial.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zccU_1K-yuhq"
   },
   "source": [
    "Next steps\n",
    "--------------\n",
    "\n",
    "- Check out the rest of Ben Trevett's tutorials using ``torchtext``\n",
    "  `here <https://github.com/bentrevett/>`__\n",
    "- Stay tuned for a tutorial using other ``torchtext`` features along\n",
    "  with ``nn.Transformer`` for language modeling via next word prediction!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Jungen tanzen mitten in der Nacht auf Pfosten.\n",
      "Translation: young dancing in the middle of the middle of a . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('pytorch-translation_tutorial.pt'))\n",
    "\n",
    "# Load the German and English spaCy tokenizers\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def translate_sentence(model, sentence, src_tokenizer, trg_tokenizer, max_length=50):\n",
    "    \n",
    "    model.eval()\n",
    "    # Tokenize the input sentence\n",
    "    tokenized_sentence = [tok.text.lower() for tok in src_tokenizer(sentence)]\n",
    "    \n",
    "    # Add start and end tokens and convert to tensor\n",
    "    tokenized_sentence = ['<bos>'] + tokenized_sentence + ['<eos>']\n",
    "    \n",
    "    src_indexes = [de_vocab([word])[0] for word in tokenized_sentence]    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(model.device)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    \n",
    "    # Create a list to store the translated words\n",
    "    trg_indexes = en_vocab(['<sos>'])\n",
    "    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(model.device)\n",
    "    \n",
    "    # Initialize variables for the decoding loop\n",
    "    for _ in range(max_length):\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "        \n",
    "        pred_token = output.max(1)[1]\n",
    "        trg_indexes.append(pred_token.item())\n",
    "        \n",
    "        trg_tensor = pred_token\n",
    "        \n",
    "        if pred_token.item() == en_vocab(['<eos>'])[0]:\n",
    "            break\n",
    "    # Convert the indices to words\n",
    "    \n",
    "    translated_sentence = [en_vocab.get_itos()[i] for i in trg_indexes]\n",
    "    \n",
    "    # Remove the start and end tokens\n",
    "    translated_sentence = translated_sentence[1:-1]\n",
    "    \n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"Jungen tanzen mitten in der Nacht auf Pfosten.\"\n",
    "# input_sentence = \"Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\"\n",
    "\n",
    "translated_sentence = translate_sentence(model, input_sentence, spacy_de, spacy_en)\n",
    "print(f'Input: {input_sentence}')\n",
    "print(f'Translation: {translated_sentence}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab(['<bos>'])[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 - GPU (ipykernel)",
   "language": "python",
   "name": "python3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
