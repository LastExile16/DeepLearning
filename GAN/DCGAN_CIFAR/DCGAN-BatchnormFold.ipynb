{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c5bdf0",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "In this notebook, you'll build a GAN using convolutional layers in the generator and discriminator. This is called a Deep Convolutional GAN, or DCGAN for short. The DCGAN architecture was first explored in 2016 and has seen impressive results in generating new images; you can read the [original paper, here](https://arxiv.org/pdf/1511.06434.pdf).\n",
    "\n",
    "You'll be training DCGAN on the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. These are color images of different classes, such as airplanes, dogs or trucks. This dataset is much more complex and diverse than the MNIST dataset and justifies the use of the DCGAN architecture.\n",
    "\n",
    "<img src='assets/cifar10_data.png' width=80% />\n",
    "\n",
    "\n",
    "So, our goal is to create a DCGAN that can generate new, realistic-looking images. We'll go through the following steps to do this:\n",
    "* Load in and pre-process the CIFAR10 dataset\n",
    "* **Define discriminator and generator networks**\n",
    "* Train these adversarial networks\n",
    "* Visualize the loss over time and some sample, generated images\n",
    "\n",
    "In this notebook, we will focus on defining the networks.\n",
    "\n",
    "#### Deeper Convolutional Networks\n",
    "\n",
    "Since this dataset is more complex than our MNIST data, we'll need a deeper network to accurately identify patterns in these images and be able to generate new ones. Specifically, we'll use a series of convolutional or transpose convolutional layers in the discriminator and generator. It's also necessary to use batch normalization to get these convolutional networks to train. \n",
    "\n",
    "Besides these changes in network structure, training the discriminator and generator networks should be the same as before. That is, the discriminator will alternate training on real and fake (generated) images, and the generator will aim to trick the discriminator into thinking that its generated images are real!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fe624",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "Here you'll build the discriminator. This is a convolutional classifier like you've built before, only without any maxpooling layers. \n",
    "* The inputs to the discriminator are 32x32x3 tensor images\n",
    "* You'll want a few convolutional, hidden layers\n",
    "* Then a fully connected layer for the output; as before, we want a sigmoid output, but we'll add that in the loss function, [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss), later\n",
    "\n",
    "<img src='assets/conv_discriminator.png' width=80%/>\n",
    "\n",
    "For the depths of the convolutional layers I suggest starting with 32 filters in the first layer, then double that depth as you add layers (to 64, 128, etc.). **Note that in the DCGAN paper, they did all the downsampling using only strided convolutional layers with no maxpooling layers.**\n",
    "\n",
    "You'll also want to use batch normalization with [nn.BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d) on each layer **except** the first convolutional layer and final, linear output layer. \n",
    "\n",
    "#### Helper `ConvBlock` module \n",
    "\n",
    "In general, each layer should look something like convolution > batch norm > leaky ReLU, and so we'll define a **custom torch Module** to put these layers together. This module will create a sequential series of a convolutional + an optional batch norm layer. \n",
    "\n",
    "Note: It is also suggested that you use a **kernel_size of 4** and a **stride of 2** for strided convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba18327",
   "metadata": {},
   "source": [
    "### First exercise\n",
    "\n",
    "Implement the `ConvBlock` module below and use it for your implementation of the `Discriminator` module. Your discriminator should take a 32x32x3 image as input and output a single logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f95563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx\n",
    "\n",
    "import tests\n",
    "import fuser\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b109556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    The discriminator model adapted from the DCGAN paper. It should only contains a few layers.\n",
    "    args:\n",
    "    - conv_dim: control the number of filters\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        ####\n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "        self.conv1 = self._block(in_channels=3, out_channels=conv_dim, kernel_size=4, bias=True, batch_norm = False) # 32x32 -> 16x16\n",
    "        self.conv2 = self._block(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=4, bias=False) # 16x16 -> 8x8\n",
    "        self.conv3 = self._block(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=4, bias=False) # 8x8 -> 4x4\n",
    "        self.conv4 = self._block(in_channels=conv_dim*4, out_channels=1, kernel_size=4, padding=0, bias=False) # 4x4 -> 1x1\n",
    "        \n",
    "        # get rid of any fully connected layer with respect to the DCGAN paper\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.fc1 = nn.Linear((4*4)*(conv_dim*4), 1)\n",
    "    \n",
    "    def _block(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int, \n",
    "                 stride: int = 2, \n",
    "                 padding: int = 1,\n",
    "                 bias: bool = False,\n",
    "                 batch_norm: bool = True):\n",
    "        \"\"\"\n",
    "        A convolutional block is made of 3 layers: Conv -> BatchNorm -> Activation.\n",
    "        args:\n",
    "        - in_channels: number of channels in the input to the conv layer\n",
    "        - out_channels: number of filters in the conv layer\n",
    "        - kernel_size: filter dimension of the conv layer\n",
    "        - batch_norm: whether to use batch norm or not\n",
    "        \"\"\"\n",
    "        if batch_norm:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                # in the DCGAN paper, they say not to use batchnorm on the first layer of descriminator and last layer of generator\n",
    "                # however, in dicriminator, the bias will be canceled out in the following layers I think! so in the end, we don't have any bias!?\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        ####\n",
    "        # IMPLEMENT HERE\n",
    "        ####      \n",
    "        x = self.conv1(x)\n",
    "        # print(f\"shape at conv1: {x.shape}\")\n",
    "        x = self.conv2(x)\n",
    "        # print(f\"shape at conv2: {x.shape}\")\n",
    "        x = self.conv3(x)\n",
    "        # print(f\"shape at conv3: {x.shape}\")\n",
    "        x = self.conv4(x)\n",
    "        # print(f\"shape at conv4: {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78558187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(128)\n",
    "print(discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9754249-f998-4c00-aee8-5424744ec460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv1): Module(\n",
       "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv2): Module(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv3): Module(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv4): Module(\n",
       "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.quantize_fx import fuse_fx\n",
    "discriminator.eval()\n",
    "fuse_fx(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf7534-066c-495b-903e-5c98f5d5cd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4f2199d-9b7a-46cd-a2a0-2c7ae3c90455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_1: 16\n",
      "layer_2: 8\n",
      "layer_3: 4\n",
      "layer_4: 1\n"
     ]
    }
   ],
   "source": [
    "image_res = 32\n",
    "# input_dim: int, padding: int, kernel: int, stride: int, layers: int\n",
    "tests.image_size_conv_output(image_res, [1,1,1,0], 4, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5ae81452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape at conv1: torch.Size([16, 128, 16, 16])\n",
      "shape at conv2: torch.Size([16, 256, 8, 8])\n",
      "shape at conv3: torch.Size([16, 512, 4, 4])\n",
      "shape at conv4: torch.Size([16, 1, 1, 1])\n",
      "Congrats, you successfully implemented your discriminator\n"
     ]
    }
   ],
   "source": [
    "tests.check_discriminator(discriminator, image_res=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b39902",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "Next, you'll build the generator network. The input will be our noise vector `z`, as before. And, the output will be a $tanh$ output, but this time with size 32x32 which is the size of our CIFAR10 images.\n",
    "\n",
    "<img src='assets/conv_generator.png' width=80% />\n",
    "\n",
    "What's new here is we'll use transpose convolutional layers to create our new images. \n",
    "* The first layer is a fully connected layer which is reshaped into a deep and narrow layer, something like 4x4x512. \n",
    "* Then, we use batch normalization and a leaky ReLU activation. \n",
    "* Next is a series of [transpose convolutional layers](https://pytorch.org/docs/stable/nn.html#convtranspose2d), where you typically halve the depth and double the width and height of the previous layer. \n",
    "* And, we'll apply batch normalization and ReLU to all but the last of these hidden layers. Where we will just apply a `tanh` activation.\n",
    "\n",
    "#### Helper `DeconvBlock` module\n",
    "\n",
    "For each of these layers, the general scheme is transpose convolution > batch norm > ReLU, and so we'll define a function to put these layers together. This function will create a sequential series of a transpose convolutional + an optional batch norm layer. We'll create these using PyTorch's Sequential container, which takes in a list of layers and creates layers according to the order that they are passed in to the Sequential constructor.\n",
    "\n",
    "Note: It is also suggested that you use a **kernel_size of 4** and a **stride of 2** for transpose convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae89eab",
   "metadata": {},
   "source": [
    "#### Second exercise\n",
    "\n",
    "Implement the `DeconvBlock` module below and use it for your implementation of the `Generator` module. Your generator should take a latent vector of dimension 128 as input and output a 32x32x3 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a15d23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    The generator model adapted from DCGAN\n",
    "    args:\n",
    "    - latent_dim: dimension of the latent vector 100x1x1\n",
    "    - conv_dim: control the number of filters in the convtranspose layers\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, conv_dim: int = 32):\n",
    "        super().__init__()\n",
    "        ####\n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "        self.conv_dim = conv_dim\n",
    "        # self.fc1 = nn.Linear(latent_dim, conv_dim*4*4, bias=False)\n",
    "        # reshape to (batch_size, conv_dim, 4, 4)\n",
    "        self.tconv1 = self._block(in_channels=latent_dim, out_channels=conv_dim*16, kernel_size=4, stride=2, padding=0, bias=False)\n",
    "        self.tconv2 = self._block(in_channels=conv_dim*16, out_channels=conv_dim*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.tconv3 = self._block(in_channels=conv_dim*8, out_channels=conv_dim*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.tconv4 = self._block(in_channels=conv_dim*4, out_channels=3, kernel_size=4, stride=2, padding=1, bias=True, batch_norm=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _block(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int, \n",
    "                 stride: int,\n",
    "                 padding: int,\n",
    "                 bias: bool = False,\n",
    "                 batch_norm: bool = True):\n",
    "        \"\"\"\n",
    "        A \"de-convolutional\" block is made of 3 layers: ConvTranspose -> BatchNorm -> Activation.\n",
    "        args:\n",
    "        - in_channels: number of channels in the input to the conv layer\n",
    "        - out_channels: number of filters in the conv layer\n",
    "        - kernel_size: filter dimension of the conv layer\n",
    "        - stride: stride of the conv layer\n",
    "        - padding: padding of the conv layer\n",
    "        - batch_norm: whether to use batch norm or not\n",
    "        \"\"\"\n",
    "        if batch_norm:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        ####\n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "        # x = self.fc1(x)\n",
    "        # x = x.view(-1, self.conv_dim, 4, 4)\n",
    "        print(f\"input {x.shape}\")\n",
    "        x = self.tconv1(x)\n",
    "        print(f\"shape at conv1: {x.shape}\")\n",
    "        x = self.tconv2(x)\n",
    "        print(f\"shape at conv2: {x.shape}\")\n",
    "        x = self.tconv3(x)\n",
    "        print(f\"shape at conv3: {x.shape}\")\n",
    "        x = self.tconv4(x)\n",
    "        print(f\"shape at conv4: {x.shape}\")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bfa140ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (tconv1): Sequential(\n",
      "    (0): ConvTranspose2d(128, 512, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (tconv2): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (tconv3): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (tconv4): Sequential(\n",
      "    (0): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(128)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "080e07e6-b424-4501-ab05-f2c754b501a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_1: 0\n",
      "layer_2: -2\n",
      "layer_3: -6\n",
      "layer_4: -14\n"
     ]
    }
   ],
   "source": [
    "image_res = 1\n",
    "# input_dim: int, padding: int, kernel: int, stride: int, layers: int\n",
    "tests.image_size_trans_conv_output(image_res, padding=[1,1,1,1], kernel = 2, stride=2, layers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "048493e6-7977-495d-916d-4f3502c7ba79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Padding calculator\n",
    "output_dim = 4  # Adjust the size as needed\n",
    "input_dim = 1\n",
    "stride=2\n",
    "kernel_size=4\n",
    "\n",
    "# Calculate the required padding to match the output size\n",
    "padding = ((output_size - 1) // 2)\n",
    "padding = ((input_dim - 1) * stride - output_dim + kernel_size) // 2\n",
    "print(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7aaf06d5-5941-415f-9c46-ac517bafe0db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "input = torch.ones(1, 1, 1, 1)\n",
    "upsample = nn.ConvTranspose2d(1, 1, kernel_size=4, stride=2, padding=0)\n",
    "output = upsample(input)\n",
    "print(f\"Output size: {output.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5331bc89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([16, 128, 1, 1])\n",
      "shape at conv1: torch.Size([16, 512, 4, 4])\n",
      "shape at conv2: torch.Size([16, 256, 8, 8])\n",
      "shape at conv3: torch.Size([16, 128, 16, 16])\n",
      "shape at conv4: torch.Size([16, 3, 32, 32])\n",
      "Congrats, you successfully implemented your discriminator\n"
     ]
    }
   ],
   "source": [
    "tests.check_generator(model=generator, latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbcddb6-0e0f-4e18-9162-def8616b42e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - GPU (ipykernel)",
   "language": "python",
   "name": "python3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
