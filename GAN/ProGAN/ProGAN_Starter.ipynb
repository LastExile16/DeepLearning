{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc03cd61",
   "metadata": {},
   "source": [
    "## Progressive Growing of GANs\n",
    "*Note: the implementation followed from this Youtube [video](https://www.youtube.com/watch?v=nkQHASviYac)*\n",
    "*Link to github [here](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/GANs/ProGAN/model.py)*\n",
    "\n",
    "By now, you probably have realized how difficult it can be to train GANs. They are fairly unstable, especially when trying to generate high-dimensional samples, such as high-resolution images! \n",
    "\n",
    "However, researchers never lack ideas to improve them and this 2017 paper made trainings of GANs more stable: *Progressive growing of GANs for improved quality, stability and variation*.\n",
    "\n",
    "The main idea behind this paper is the following: since training GANs on smaller images is easier, we can progressively grow the network and the generated images' dimensions to make training easier for the network. It is illustrated by the figure below:\n",
    "\n",
    "<img src='assets/progan2.png' width=70% />\n",
    "\n",
    "\n",
    "### Layer fading \n",
    "\n",
    "Each level, or depth, is training for a certain number of epochs (e.g., 10 epochs). Then a new layer is added in the discriminator and the generator and we start training with these additional layers. However, when a new layer is added, it is faded in smoothly, as described by the following figure:\n",
    "\n",
    "<img src='assets/layer_fading2.png' width=70% />\n",
    "\n",
    "The `toRGB` and `fromRGB` layers are the layers projecting the feature vector to the RGB space (HxWx3) and the layer doing the opposite, respectively. \n",
    "\n",
    "Let's look at the example:\n",
    "* **(a)** The network is currrently training at 16x16 resolution, meaning that the generated images are 16x16x3\n",
    "* **(b)** We are adding two new layers to train at 32x32 resolution. However, we are fading in the new layers by doing the following:\n",
    "    * For the generator, we take the output of the 16x16 layer and use nearest neighbor image resize to double its resolution to 32x32. The same output will also be fed to the 32x32 layer. Then we calculate the output of the network by doing a weighted sum of $(1- \\alpha)$ the upsampled 16x16 image and $\\alpha$ the 32x32 layer output. \n",
    "    * For the discriminator, we do something similar but to reduce the resolution, we use an average pooling layer\n",
    "    * The network trains for N epochs at each resolution. During the first $N/2$ epochs, we start with $/alpha = 0$ and increase alpha linearly to $/alpha = 1$. Then we train for the remaining $N/2$ epochs with $/alpha = 1$.\n",
    "* **(c)** The network is now training at 32x32 resolution\n",
    "\n",
    "### Architecture used in the paper.\n",
    "**note**: \n",
    "  * the first conv layer of generator is transpose to go from 1x1 to 4x4\n",
    "  * The other blocks are the same for generator and discriminator except that in discriminator they don't use pixelNorm.\n",
    "  * After each block we should have an RGB in between blocks.\n",
    "  * In the end, we only need the RGB in the last layer as ones in between are for progressive training only.\n",
    "<img src='assets/progan_architecture.png' width=80% />\n",
    "\n",
    "**Note:** In the paper, the authors are using a new type of normalization, called PixelNormalization. I encourage you to read the paper but for the sake of simplicity, I did not add any normalization here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c425ebcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7231b954-55ba-4949-94e3-261c0a929597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 512 x4 -> 256 -> 128 -> 64 -> 32 -> 16 -> 1\n",
    "factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f9e167-0638-4721-8172-d22cd809e3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    \"\"\" \n",
    "    Equalized learning rate: scale the weights by a factor.\n",
    "        in the paper, the authors claim that the learning rate can \n",
    "    be big for some weights while small for some other weights.\n",
    "    so, having the dynamic range weights will make sure they require the same learning rate;\n",
    "    W = W * sqrt(2/(k**2)*in_channels) for every convblock at farward\n",
    "    \"\"\"    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain/(in_channels*kernel_size**2)) ** 0.5 \n",
    "        # the bias should not be scaled.\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "        \n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return  self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c016be1a-038e-4d94-b4c3-7caaa2d85bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WSConvTranspose2d(nn.Module):\n",
    "    \"\"\" \n",
    "    Equalized learning rate: scale the weights by a factor.\n",
    "        in the paper, the authors claim that the learning rate can \n",
    "    be big for some weights while small for some other weights.\n",
    "    so, having the dynamic range weights will make sure they require the same learning rate;\n",
    "    W = W * sqrt(2/(k**2)*in_channels) for every convblock at farward\n",
    "    \"\"\"    \n",
    "    def __init__(self, z_dim, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.tconv = nn.ConvTranspose2d(z_dim, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain/(z_dim*kernel_size**2)) ** 0.5 \n",
    "        # the bias should not be scaled.\n",
    "        self.bias = self.tconv.bias\n",
    "        self.tconv.bias = None\n",
    "        \n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.tconv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return  self.tconv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf684d6-1b9c-483e-aada-f1ef20042509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8daa2560-ce34-4f75-a20c-6727022d65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelNorm=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pn = use_pixelNorm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        \n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40efb08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeneratorFirstBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This block follows the ProGan paper implementation.\n",
    "    Takes the latent vector and creates feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        # initial block \n",
    "        self.conv0 = nn.ConvTranspose2d(latent_dim, 512, kernel_size=4) # 1x1 -> 4x4\n",
    "        self.conv1 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # 4x4 -> 4x4\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is a (batch_size, latent_dim) latent vector, we need to turn it into a feature map\n",
    "        x = torch.unsqueeze(torch.unsqueeze(x, -1), -1) # batch_size, latent_dim, H, W\n",
    "        x = self.conv0(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7416a",
   "metadata": {},
   "source": [
    "Using the above two blocks, you can implement the Generator module. The end resolution that we want to reach is 512x512 and we will start at a 4x4 resolution. \n",
    "\n",
    "\n",
    "#### init\n",
    "The `__init__` method should contain enough blocks to work at full resolution. We are only instantiating the generator once! So you will need to:\n",
    "* Create one GeneratorFirstBlock module\n",
    "* Create enough GeneratorBlocks modules such that the final resolution is 512x512\n",
    "* Create one `toRGB` layer per resolution. \n",
    "\n",
    "The number of filters in each layer is controlled by the `num_filters` function below.\n",
    "\n",
    "\n",
    "#### forward\n",
    "\n",
    "The forward method does the following:\n",
    "* Takes the latent vector, the current resolution and `alpha` as input \n",
    "* Runs the latent vector through the different blocks and performs `alpha` fading\n",
    "\n",
    "\n",
    "In the original paper, the number of filters of convolution layers increases with depth. The `num_filters` function below will help you progammatically increase the number of filters based on the stage (or depth) of the generator. A depth of 1 correspond to 4x4 resolution, a depth of 2 to an 8x8 resolution etc. \n",
    "\n",
    "* you can the torch `interpolate` function to double the resolution of an image\n",
    "* you can use the `np.log2` function to map the resolution of the input image to a \"depth\" (or stage) level. For example, `np.log2(512) = 9` and `np.log2(4)` = 2.\n",
    "* when training at 4x4 resolution, you should not perform $\\alpha-$fading.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390af6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tests\n",
    "\n",
    "from torch.nn.functional import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23461cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_filters(stage: int, \n",
    "                fmap_base: int = 8192,\n",
    "                fmap_decay: float = 1.0,\n",
    "                fmap_max: int = 512): \n",
    "    \"\"\"\n",
    "    A small helper function to compute the number of filters for conv layers based on the stage/depth,\n",
    "    stage = log2(resolution)\n",
    "    From the original repo https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py#L252\n",
    "    \"\"\"\n",
    "    return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bdb2e29-4367-4be4-923c-a75791fdcc27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_filters(5)\n",
    "int(np.log2(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83303295-70f5-4fa1-8022-b39d84f7df1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            WSConvTranspose2d(z_dim, in_channels, kernel_size=4, stride=1, padding=0), # 1x1 -> 4x4\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n",
    "        \n",
    "        for i in range(len(factors)-1):\n",
    "            # factors[i] -> factors[i+1]\n",
    "            conv_in_c = int(in_channels*factors[i])\n",
    "            conv_out_c = int(in_channels*factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "            \n",
    "        \n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha*generated + (1-alpha)*upscaled)\n",
    "        \n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "        \n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "        \n",
    "        last_upscaled = self.rgb_layers[steps-1](upscaled) # take the last upscaled result and run it through rgb\n",
    "        final_output = self.rgb_layers[steps](out) # take the last generated conv image and run it through rgb\n",
    "        \n",
    "        return self.fade_in(alpha, last_upscaled, final_output) # final result is a mixture of upscaled and actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc01ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z_dim=128, in_channels=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624313f8-6fd7-40f2-b9d3-bc6f86451309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelNorm=False))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n",
    "        # did this to \"mirror\" the generator initial_rgb\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        # this is the block for 4x4 input size\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),  # we use this instead of linear layer\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "        # use the final block\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        # convert from rgb as initial step, this will depend on\n",
    "        # the image size (each will have it's on rgb layer)\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
    "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        # the fade_in is done first between the downscaled and the input\n",
    "        # this is opposite from the generator\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3eeeebd-67dd-4e09-b3d9-5745738b361c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "Success! At img size: 4\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 8\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 16\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 32\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 64\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 128\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 256\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 512\n",
      "torch.Size([1, 1])\n",
      "Success! At img size: 1024\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Z_DIM = 100\n",
    "    IN_CHANNELS = 512\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "    critic = Discriminator(IN_CHANNELS, img_channels=3)\n",
    "\n",
    "    for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "        num_steps = int(np.log2(img_size / 4))\n",
    "        x = torch.randn((1, Z_DIM, 1, 1))\n",
    "        z = gen(x, 0.5, steps=num_steps)\n",
    "        assert z.shape == (1, 3, img_size, img_size)\n",
    "        out = critic(z, alpha=0.5, steps=num_steps)\n",
    "        print(out.shape)\n",
    "        assert out.shape == (1, 1)\n",
    "        print(f\"Success! At img size: {img_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4075bc9-933e-4f17-93b2-f29d9b5d3d47",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a565a0b-477c-49a3-821c-32cd68677244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:51:52.089620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 14:51:52.954797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-13 14:51:52.954903: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-13 14:51:52.954910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image size: 128\n",
      "Epoch [1/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:25<00:00,  3.71it/s, gp=0.119, loss_critic=-11.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0909, loss_critic=-7]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.73it/s, gp=0.17, loss_critic=3.36]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:25<00:00,  3.71it/s, gp=0.0745, loss_critic=-4.65] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0935, loss_critic=-5.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:24<00:00,  3.72it/s, gp=0.0524, loss_critic=-4.98] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0582, loss_critic=-5.61] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0443, loss_critic=-2.58] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0514, loss_critic=-1.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0521, loss_critic=-3.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0593, loss_critic=-1.25]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0504, loss_critic=1.2]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0268, loss_critic=-2.74]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0394, loss_critic=-2.42]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [08:23<00:00,  3.72it/s, gp=0.0381, loss_critic=-2.76]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 1774/1875 [07:56<00:26,  3.74it/s, gp=0.0385, loss_critic=-3.46] "
     ]
    }
   ],
   "source": [
    "\"\"\" Training of ProGAN using WGAN-GP loss\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import (\n",
    "    gradient_penalty,\n",
    "    plot_to_tensorboard,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    generate_examples,\n",
    ")\n",
    "# from model import Discriminator, Generator\n",
    "from math import log2\n",
    "from tqdm import tqdm\n",
    "import config\n",
    "\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "\n",
    "\n",
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(config.CHANNELS_IMG)],\n",
    "                [0.5 for _ in range(config.CHANNELS_IMG)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = config.BATCH_SIZES[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=config.DATASET, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset\n",
    "\n",
    "\n",
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    loader,\n",
    "    dataset,\n",
    "    step,\n",
    "    alpha,\n",
    "    opt_critic,\n",
    "    opt_gen,\n",
    "    tensorboard_step,\n",
    "    writer,\n",
    "    scaler_gen,\n",
    "    scaler_critic,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(config.DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n",
    "        # which is equivalent to minimizing the negative of the expression\n",
    "        noise = torch.randn(cur_batch_size, config.Z_DIM, 1, 1).to(config.DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, alpha, step)\n",
    "            critic_real = critic(real, alpha, step)\n",
    "            critic_fake = critic(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(critic, real, fake, alpha, step, device=config.DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + config.LAMBDA_GP * gp\n",
    "                + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        # Update alpha and ensure less than 1\n",
    "        alpha += cur_batch_size / (\n",
    "            (config.PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(config.FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialize gen and disc, note: discriminator should be called critic,\n",
    "    # according to WGAN paper (since it no longer outputs between [0, 1])\n",
    "    # but really who cares..\n",
    "    gen = Generator(\n",
    "        config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG\n",
    "    ).to(config.DEVICE)\n",
    "    critic = Discriminator(\n",
    "        config.IN_CHANNELS, img_channels=config.CHANNELS_IMG\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    # initialize optimizers and scalers for FP16 training\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    opt_critic = optim.Adam(\n",
    "        critic.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99)\n",
    "    )\n",
    "    scaler_critic = torch.cuda.amp.GradScaler()\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # for tensorboard plotting\n",
    "    writer = SummaryWriter(f\"logs/gan1\")\n",
    "\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN, gen, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_CRITIC, critic, opt_critic, config.LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "    tensorboard_step = 0\n",
    "    # start at step that corresponds to img size that we set in config\n",
    "    step = int(log2(config.START_TRAIN_AT_IMG_SIZE / 4))\n",
    "    for num_epochs in config.PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5  # start with very low alpha\n",
    "        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n",
    "        print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train_fn(\n",
    "                critic,\n",
    "                gen,\n",
    "                loader,\n",
    "                dataset,\n",
    "                step,\n",
    "                alpha,\n",
    "                opt_critic,\n",
    "                opt_gen,\n",
    "                tensorboard_step,\n",
    "                writer,\n",
    "                scaler_gen,\n",
    "                scaler_critic,\n",
    "            )\n",
    "\n",
    "            if config.SAVE_MODEL:\n",
    "                save_checkpoint(gen, opt_gen, filename=config.CHECKPOINT_GEN)\n",
    "                save_checkpoint(critic, opt_critic, filename=config.CHECKPOINT_CRITIC)\n",
    "\n",
    "        step += 1  # progress to the next img size\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d12de-1ac3-4168-88e5-f80fa31cee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - GPU (ipykernel)",
   "language": "python",
   "name": "python3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
